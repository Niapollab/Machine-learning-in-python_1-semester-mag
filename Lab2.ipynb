{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2 \"Классификация текста\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Sequence\n",
    "import fasttext\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "ONLY_WORDS = re.compile(r'[^\\sa-z]', re.I)\n",
    "ONLY_DIGITS = re.compile(r'[^\\d]')\n",
    "ALL_SPACE_SYMBOLS = re.compile(r'\\s+')\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = ONLY_WORDS.sub('', text)\n",
    "    text = ALL_SPACE_SYMBOLS.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_cls_data(filename: str, feature: Sequence[str], label: Sequence[int]) -> None:\n",
    "    with open(filename, 'w') as file:\n",
    "        for each_feature, each_label in zip(feature, label):\n",
    "            file.writelines(f'__label__{each_label} {each_feature}\\n')\n",
    "\n",
    "\n",
    "def lemmatize(sentence: str) -> str:\n",
    "    return ' '.join([LEMMATIZER.lemmatize(w) for w in sentence.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/indian_fake_news.tar.gz')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].notna()]\n",
    "df['text'] = df['text'].apply(sanitize_text)\n",
    "df['label'] = pd.factorize(df['label'], sort=True)[0]\n",
    "df.rename({'label': 'is_real'}, inplace=True, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_analysis = Counter(df['text'].str.cat())  # type: ignore\n",
    "\n",
    "dec_sorted = [*sorted(frequency_analysis.items(), key=lambda x: x[1])]\n",
    "keys = [str(k) for k, _ in dec_sorted]\n",
    "values = [v for _, v in dec_sorted]\n",
    "\n",
    "plt.barh(keys, values)\n",
    "plt.xlabel('Частота')\n",
    "plt.ylabel('Символ')\n",
    "plt.title('Частотный анализ символов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_real'].value_counts().plot(kind='barh', title='Анализ значений классификации', xlabel='Число вхождений', ylabel='Метка')\n",
    "plt.show()\n",
    "\n",
    "df.drop_duplicates()['is_real'].value_counts().plot(kind='barh', title='Анализ значений классификации (только уникальные)', xlabel='Число вхождений', ylabel='Метка')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 2e-1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train, test = train_test_split(df, test_size=TRAIN_PROPORTION, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILENAME = 'train.txt'\n",
    "TEST_FILENAME = 'test.txt'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train['text'], test['text'], train['is_real'], test['is_real']\n",
    "\n",
    "save_cls_data(TRAIN_FILENAME, X_train, y_train)\n",
    "save_cls_data(TEST_FILENAME, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 1.0\n",
    "WORD_NGRAMS = 3\n",
    "\n",
    "fasttext_model = fasttext.train_supervised('train.txt', epoch=EPOCHS, lr=LR, wordNgrams=WORD_NGRAMS)\n",
    "y_pred = X_test.apply(lambda x: int(ONLY_DIGITS.sub('', fasttext_model.predict(x)[0][0])))  # type: ignore\n",
    "\n",
    "fasttext_out = (y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model = CatBoostClassifier(cat_features=['text'], random_state=RANDOM_SEED, verbose=0)\n",
    "catboost_model.fit(pd.DataFrame(X_train, columns=['text']), y_train)\n",
    "\n",
    "y_pred = catboost_model.predict(pd.DataFrame(X_test, columns=['text']))\n",
    "catboost_out = (y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_VECTORIZER = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords.words('english'))\n",
    "\n",
    "X_train, X_test = (series.apply(lemmatize) for series in (X_train, X_test))\n",
    "X_train, X_test = COUNT_VECTORIZER.fit_transform(X_train), COUNT_VECTORIZER.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 42\n",
    "MAX_DEPTH = 9\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_SEED) \\\n",
    "    .fit(X_train, y_train)\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "random_forest_out = (y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_true, y_pred in (fasttext_out, catboost_out, random_forest_out):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt = ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_true, y_pred in (fasttext_out, catboost_out, random_forest_out):\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
